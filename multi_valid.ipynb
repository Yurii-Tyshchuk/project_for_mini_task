{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import glob\n",
    "import decimal\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_file(path: str, encoding: str, is_lower=False):\n",
    "    list_of_paragraph = []\n",
    "    buffer = []\n",
    "    with open(path, encoding=encoding) as file:\n",
    "        regex = re.compile(\n",
    "            r'(((?<!Статья )(?<!^)(?<!(\\.|\\s))(?<!     )[а-яА-Я\\d]{2,})([\\.]{0,1}[\\]\\)\\\"]{0,2}[\\.\\;\\:]{1}(\\s|$)))',\n",
    "            flags=re.IGNORECASE)\n",
    "        text = file.read()\n",
    "\n",
    "        if is_lower:\n",
    "            text = text.lower()\n",
    "        offset = 0\n",
    "        buf = ''\n",
    "        for ind, value in enumerate(regex.split(text, maxsplit=0)):\n",
    "            if ind == 0 + offset:\n",
    "                buf = value\n",
    "            if ind == 1 + offset:\n",
    "                list_of_paragraph.append((buf + value).strip())\n",
    "                offset += 6\n",
    "\n",
    "        second_regex = re.compile(r'([^\\dгст][\\.\\;\\:](?=\\s|$))', flags=re.IGNORECASE)\n",
    "\n",
    "        for value1 in list_of_paragraph:\n",
    "            buf = ''\n",
    "            for value in second_regex.split(value1, maxsplit=0):\n",
    "                if value == '':\n",
    "                    continue\n",
    "\n",
    "                if len(value) > 2 and buf != '':\n",
    "                    buffer.append(buf.strip())\n",
    "                    buf = value\n",
    "                elif len(value) > 2:\n",
    "                    buf = value\n",
    "                elif len(value) <= 2:\n",
    "                    buffer.append((buf + value).strip())\n",
    "                    buf = ''\n",
    "    return buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting file paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "list_of_path = list(filter(lambda x: not '1ДИ' in str(x), glob.glob(\"docs/*/*.txt\")))\n",
    "\n",
    "list_of_instruction_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список ДИ'), list_of_path))\n",
    "list_of_external_doc_paths = list(filter(lambda x: str(x).startswith('docs\\\\Список внешних'), list_of_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation of sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def remove_all_specific_symbols(text: str):\n",
    "    text = re.sub(r'[\\.\\,\\;\\:]', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    bad_symbols = [')', '*']\n",
    "    for symbol in bad_symbols:\n",
    "        if text.startswith(symbol):\n",
    "            text = text[1:]\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    text = re.sub(r'(\\d+)(?:\\.)?', '', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def valid(text: str) -> bool:\n",
    "    def mini_valid(x: str):\n",
    "        list_of_specific_symbols = ['.', ',', ':', ';']\n",
    "        for sym in list_of_specific_symbols:\n",
    "            if sym in x:\n",
    "                return len([y for y in re.split(r'\\s', ' '.join(x.split(sym, maxsplit=0))) if len(y) > 1]) > 2\n",
    "        return True\n",
    "\n",
    "    flag: bool = len(list(filter(lambda x: len(str(x)) > 0 and mini_valid(x), re.split(r'\\s', text, maxsplit=0)))) > 2\n",
    "    return len(text) > 3 and flag"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.83 s\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_of_external_docs = []\n",
    "list_of_instructions = []\n",
    "\n",
    "in_lower_case = True\n",
    "\n",
    "for path_to_doc in list_of_instruction_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [clean(x) for x in buf_list]\n",
    "    buf_list = [remove_all_specific_symbols(x) for x in buf_list if valid(x)]\n",
    "    list_of_instructions.append(buf_list)\n",
    "\n",
    "for path_to_doc in list_of_external_doc_paths:\n",
    "    buf_list = read_file(path_to_doc, 'windows-1251', in_lower_case)\n",
    "    buf_list = [clean(x) for x in buf_list]\n",
    "    buf_list = [remove_all_specific_symbols(x) for x in buf_list if valid(x)]\n",
    "    list_of_external_docs.append(buf_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "download_first_model = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "embed = None\n",
    "size = 512\n",
    "if download_first_model:\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "else:\n",
    "    size = 768\n",
    "    embed = SentenceTransformer('symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', device='cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "bad_words = []\n",
    "bad_words2 = []\n",
    "\n",
    "with open('./stop.txt', encoding='windows-1251') as file:\n",
    "    for text in file.readlines():\n",
    "        if text.endswith('\\n'):\n",
    "            if download_first_model:\n",
    "                bad_words.append(embed(remove_all_specific_symbols(text[:-1])))\n",
    "            else:\n",
    "                bad_words.append(normalize(embed.encode([remove_all_specific_symbols(text[:-1])])))\n",
    "\n",
    "            bad_words2.append(remove_all_specific_symbols(text[:-1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from instructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_instruction_embeddings = []\n",
    "\n",
    "for doc in list_of_instructions:\n",
    "    buffer = np.empty((0, size), dtype=\"float32\")\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            # buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "            encoded_batch = embed.encode(batch)\n",
    "            buffer = np.concatenate((buffer, normalize(encoded_batch)), axis=0)\n",
    "\n",
    "    list_of_instruction_embeddings.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting embeddings from external docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 47.6 s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 64\n",
    "list_of_embeddings_of_external_docs = []\n",
    "\n",
    "for doc in list_of_external_docs:\n",
    "    buffer = np.empty((0, size), dtype=\"float32\")\n",
    "    for i in range(0, len(doc), BATCH_SIZE):\n",
    "        batch = doc[i:i + BATCH_SIZE]\n",
    "        if download_first_model:\n",
    "            buffer = np.concatenate((buffer, embed(batch)), axis=0)\n",
    "        else:\n",
    "            # buffer = np.concatenate((buffer, embed.encode(batch)), axis=0)\n",
    "            encoded_batch = embed.encode(batch)\n",
    "            buffer = np.concatenate((buffer, normalize(encoded_batch)), axis=0)\n",
    "\n",
    "    list_of_embeddings_of_external_docs.append(buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing similarity matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "list_for_df = []\n",
    "exclude_df = pd.DataFrame(columns=[\n",
    "    'sentence_number_from_instruction',\n",
    "    'sentence_number_from_external_doc',\n",
    "    'value',\n",
    "    'stop_word',\n",
    "    'text_of_instruction',\n",
    "    'text_of_external_doc',\n",
    "    'instruction_filename',\n",
    "    'external_doc_filename'\n",
    "])\n",
    "for index, doc1 in enumerate(list_of_instruction_embeddings):\n",
    "    buffer = {\n",
    "        'instruction_filename': list_of_instruction_paths[index]\n",
    "    }\n",
    "    for ind, doc2 in enumerate(list_of_embeddings_of_external_docs):\n",
    "        buffer['external_doc_filename'] = list_of_external_doc_paths[ind]\n",
    "        similarity_matrix = np.inner(doc1, doc2)\n",
    "\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            flag = False\n",
    "            for word_index, word in enumerate(bad_words):\n",
    "                if np.max(np.inner(word, doc1[i])) >= 0.95:\n",
    "                    flag = True\n",
    "                    exclude_df = exclude_df.append({\n",
    "                        'sentence_number_from_instruction': i,\n",
    "                        'sentence_number_from_external_doc': None,\n",
    "                        'value': np.max(np.inner(word, doc1[i])),\n",
    "                        'stop_word': bad_words2[word_index],\n",
    "                        'text_of_instruction': list_of_instructions[index][i],\n",
    "                        'text_of_external_doc': None,\n",
    "                        'instruction_filename': list_of_instruction_paths[index],\n",
    "                        'external_doc_filename': None\n",
    "                    }, ignore_index=True)\n",
    "                    break\n",
    "                if np.max(np.inner(word, doc2[int(np.argmax(row))])) >= 0.95:\n",
    "                    flag = True\n",
    "                    exclude_df = exclude_df.append({\n",
    "                        'sentence_number_from_instruction': None,\n",
    "                        'sentence_number_from_external_doc': int(np.argmax(row)),\n",
    "                        'value': np.max(np.inner(word, doc2[int(np.argmax(row))])),\n",
    "                        'stop_word': bad_words2[word_index],\n",
    "                        'text_of_instruction': None,\n",
    "                        'text_of_external_doc': list_of_external_docs[ind][int(np.argmax(row))],\n",
    "                        'instruction_filename': None,\n",
    "                        'external_doc_filename': list_of_external_doc_paths[ind]\n",
    "                    }, ignore_index=True)\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            buffer['sentence_number_from_instruction'] = i\n",
    "            buffer['sentence_number_from_external_doc'] = int(np.argmax(row))\n",
    "            buffer['value'] = np.max(row)\n",
    "            buffer['text_of_instruction'] = list_of_instructions[index][i]\n",
    "            buffer['text_of_external_doc'] = list_of_external_docs[ind][int(np.argmax(row))]\n",
    "\n",
    "            list_for_df.append(buffer.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exclude_df.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'sentence_number_from_instruction',\n",
    "        'sentence_number_from_external_doc',\n",
    "        'value',\n",
    "        'text_of_instruction',\n",
    "        'instruction_filename',\n",
    "        'text_of_external_doc',\n",
    "        'external_doc_filename'\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "import importlib\n",
    "import thread\n",
    "\n",
    "importlib.reload(thread)\n",
    "\n",
    "pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "\n",
    "results = pool.map(thread.add_rows, [list_for_df[i:i + 256] for i in range(0, len(list_for_df), 256)])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "new_df = pd.concat(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_df.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for index, row in enumerate(list_for_df):\n",
    "#     new_df = new_df.append({\n",
    "#         'sentence_number_from_instruction': row['sentence_number_from_instruction'],\n",
    "#         'sentence_number_from_external_doc': row['sentence_number_from_external_doc'],\n",
    "#         'value': row['value'],\n",
    "#         'text_of_instruction': row['text_of_instruction'],\n",
    "#         'instruction_filename': row['instruction_filename'],\n",
    "#         'text_of_external_doc': row['text_of_external_doc'],\n",
    "#         'external_doc_filename': row['external_doc_filename'],\n",
    "#     }, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"embeddings.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    new_df.to_excel(writer, 'good', engine='xlsxwriter')\n",
    "    sheets_good = writer.sheets['good']\n",
    "    sheets_good.autofilter(0, 0, new_df.shape[0], new_df.shape[1])\n",
    "    # sheets_good.auto_filter.ref = sheets_good.dimensions\n",
    "    print(\"Файл создан\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"stop_words.xlsx\", engine=\"xlsxwriter\") as writer2:\n",
    "    exclude_df.to_excel(writer2, 'good', engine='xlsxwriter')\n",
    "    sheets_good = writer2.sheets['good']\n",
    "    sheets_good.autofilter(0, 0, exclude_df.shape[0], exclude_df.shape[1])\n",
    "    # sheets_good.auto_filter.ref = sheets_good.dimensions\n",
    "    print(\"Файл создан\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
